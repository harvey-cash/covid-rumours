{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Pipeline\n",
    "## User interface, from running queries on elastic, all the way through to producing annotation forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Query Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TransportError",
     "evalue": "TransportError(504, '<html>\\r\\n<head><title>504 Gateway Time-out</title></head>\\r\\n<body>\\r\\n<center><h1>504 Gateway Time-out</h1></center>\\r\\n<hr><center>nginx/1.17.10</center>\\r\\n</body>\\r\\n</html>\\r\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTransportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6be2105c1c35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtweet_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_rumours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mtweet_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_from_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"text\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"category\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\SHEFFIELD\\2020 - 2021\\Darwin Project\\covid-rumours\\querytweets\\queries.py\u001b[0m in \u001b[0;36mrandom_rumours\u001b[1;34m(amount, categories)\u001b[0m\n\u001b[0;32m     71\u001b[0m     }\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     result = elastic_client.search(index=\"covid19_misinfo_index\", body=query_body, size=amount, request_timeout=999999,\n\u001b[0m\u001b[0;32m     74\u001b[0m                                    _source=\"tweet_id\")  #\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\elasticsearch\\client\\utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\elasticsearch\\client\\__init__.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, body, index, doc_type, params, headers)\u001b[0m\n\u001b[0;32m   1656\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"from\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"from_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1658\u001b[1;33m         return self.transport.perform_request(\n\u001b[0m\u001b[0;32m   1659\u001b[0m             \u001b[1;34m\"POST\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m             \u001b[0m_make_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_search\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\elasticsearch\\transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[0;32m    388\u001b[0m                     \u001b[1;31m# raise exception on last retry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\elasticsearch\\transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m                 status, headers_response, data = connection.perform_request(\n\u001b[0m\u001b[0;32m    359\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\elasticsearch\\connection\\http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             )\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         self.log_request_success(\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\elasticsearch\\connection\\base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[1;34m(self, status_code, raw_data)\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Undecodable raw error response from server: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[0m\u001b[0;32m    316\u001b[0m             \u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         )\n",
      "\u001b[1;31mTransportError\u001b[0m: TransportError(504, '<html>\\r\\n<head><title>504 Gateway Time-out</title></head>\\r\\n<body>\\r\\n<center><h1>504 Gateway Time-out</h1></center>\\r\\n<hr><center>nginx/1.17.10</center>\\r\\n</body>\\r\\n</html>\\r\\n')"
     ]
    }
   ],
   "source": [
    "from querytweets import queries\n",
    "\n",
    "categories = [\n",
    "    \"Public authority actions, policy, and communications\",\n",
    "    \"Community spread and impact\",\n",
    "    \"Medical advice and self-treatments\",\n",
    "    \"Claims about prominent actors\",\n",
    "    \"Conspiracy theories\",\n",
    "    \"Virus transmission\",\n",
    "    \"Virus origin and properties\",\n",
    "    \"Public preparedness, protests, and civil disobedience\",\n",
    "    \"Vaccines, medical treatments, and tests\"\n",
    "]\n",
    "\n",
    "amount = 10\n",
    "\n",
    "tweets = []\n",
    "for category in categories:\n",
    "    tweet_ids = queries.random_rumours(amount, [category])\n",
    "    tweet_texts = queries.text_from_id(tweet_ids)\n",
    "    tweets = tweets + [{\"id\": tweet_id, \"text\": tweet_texts[i], \"category\": category} for i, tweet_id in enumerate(tweet_ids)]\n",
    "\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Turns tweets into term-frequency dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from claimretrieval import tweet_query\n",
    "\n",
    "tweet_queries = [{\n",
    "    \"id\": tweet[\"id\"],\n",
    "    \"query\": tweet_query.construct_query(tweet[\"text\"]), \n",
    "    \"text\": tweet[\"text\"],\n",
    "    \"category\": tweet[\"category\"]\n",
    "} for tweet in tweets]\n",
    "\n",
    "print(tweet_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create claim retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from claimretrieval import claim_index\n",
    "from claimretrieval.retriever import Retriever\n",
    "\n",
    "# Dictionary of { category: index } where each index is { word: {doc_id: frequency} }\n",
    "category_indices = claim_index.construct_indices(categories, \"./claimretrieval/IndexClaimCategory.csv\")\n",
    "\n",
    "category_retrievers = {category: Retriever(category_indices[category]) for category in categories}\n",
    "\n",
    "print(category_retrievers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieve claim shortlist for each tweet, for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "claim_df = pd.read_csv(\"./claimretrieval/IndexClaimCategory.csv\")\n",
    "claim_df.dropna(axis='columns', how='all', inplace=True)  # Drop any columns that are all N/A\n",
    "claim_df.dropna(axis='index', how='any', inplace=True)  # Drop any rows that have at least one N/A\n",
    "\n",
    "def claim_text_from_ids(claim_ids):\n",
    "    return [row['Claim'] for i, row in df.iterrows() if int(row['Index']) in claim_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_shortlists = []\n",
    "\n",
    "for tweet_query in tweet_queries:\n",
    "    tweet_id = tweet_query[\"id\"]\n",
    "    query = tweet_query[\"query\"]\n",
    "    text = tweet_query[\"text\"]\n",
    "    cat = tweet_query[\"category\"]\n",
    "    \n",
    "    # Most relevant claims shortlist\n",
    "    shortlist = []\n",
    "    for category in categories:\n",
    "        claim_indices = category_retrievers[category].shortlist(query)\n",
    "        claim_texts = claim_text_from_ids(claim_indices)\n",
    "        shortlist = shortlist + [{\"rumourID\": claim_id, \"category\": category, \"description\": claim_text}\n",
    "                                for (claim_id, claim_text) in zip(claim_indices, claim_texts)]\n",
    "    \n",
    "    tweet_shortlists.append({\"tweet_id\": tweet_id, \"content\": text, \"shortlist\": shortlist, \"category\": cat})\n",
    "    \n",
    "print(tweet_shortlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Produce JSONs for the annotation form generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tweetsToAnnotate = { \"tweetSample\": }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use the annotation form generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
