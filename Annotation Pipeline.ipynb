{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Pipeline\n",
    "## User interface, from running queries on elastic, all the way through to producing annotation forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct queries from claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from querytweets import queries\n",
    "from claimretrieval import tweet_query\n",
    "\n",
    "claim_df = pd.read_csv(\"./claimretrieval/IndexClaimCategory.csv\")\n",
    "claim_df.dropna(axis='columns', how='all', inplace=True)  # Drop any columns that are all N/A\n",
    "claim_df.dropna(axis='index', how='any', inplace=True)  # Drop any rows that have at least one N/A\n",
    "\n",
    "# ToDo: For random sample of claims of different categories?\n",
    "row_index = 5\n",
    "\n",
    "# Get query\n",
    "claim_id = claim_df.iloc[row_index]['Index']\n",
    "claim_category = claim_df.iloc[row_index]['Category']\n",
    "claim_text = claim_df.iloc[row_index]['Claim']\n",
    "\n",
    "#claim_text = \"president Donald Trump white house drink hydroxychloroquine inject bleach treat coronavirus signed bill vaccine 5G\"\n",
    "claim_text = \"Queen Elizabeth tests positive for COVID-19\"\n",
    "\n",
    "claim_query = tweet_query.construct_query(claim_text) # Term frequency dict\n",
    "print(claim_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Query Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query elastic for tweets based on claims\n",
    "# ToDo: Multiple claims\n",
    "ids_and_scores = queries.search_for_terms(100, claim_query)\n",
    "\n",
    "scores = [score for twid, score in ids_and_scores]\n",
    "tweet_ids = [twid for twid, score in ids_and_scores]\n",
    "\n",
    "tweet_texts = queries.text_from_id(tweet_ids)\n",
    "\n",
    "all_tweets = [{\"id\": tweet_id, \"text\": tweet_text} for tweet_id, tweet_text in zip(tweet_ids, tweet_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(range(len(scores)), scores)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter repeats\n",
    "# ToDo: Account for high similarity...\n",
    "unique_texts = []\n",
    "tweets = []\n",
    "for tweet in all_tweets:\n",
    "    if tweet[\"text\"] not in unique_texts:\n",
    "        unique_texts.append(tweet[\"text\"])\n",
    "        tweets.append(tweet)\n",
    "\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Turns tweets into term-frequency dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from claimretrieval import tweet_query\n",
    "\n",
    "tweet_queries = [{\n",
    "    \"id\": tweet[\"id\"],\n",
    "    \"query\": tweet_query.construct_query(tweet[\"text\"]), \n",
    "    \"text\": tweet[\"text\"]\n",
    "} for tweet in tweets]\n",
    "\n",
    "print(tweet_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet_queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create claim retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from claimretrieval import claim_index\n",
    "from claimretrieval.retriever import Retriever\n",
    "\n",
    "index = claim_index.construct_index(file_path=\"./claimretrieval/IndexClaimCategory.csv\")\n",
    "\n",
    "retriever = Retriever(category_index)\n",
    "\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieve claim shortlist for each tweet, for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "claim_df = pd.read_csv(\"./claimretrieval/IndexClaimCategory.csv\")\n",
    "claim_df.dropna(axis='columns', how='all', inplace=True)  # Drop any columns that are all N/A\n",
    "claim_df.dropna(axis='index', how='any', inplace=True)  # Drop any rows that have at least one N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_claim_ids = []\n",
    "known_claims = {}\n",
    "\n",
    "for tweet_query in tweet_queries:\n",
    "    claim_indices = category_retriever.shortlist(tweet_query[\"query\"])\n",
    "    shortlist = [{\"rumourID\": int(row['Index']), \"category\": row['Category'], \"description\": row['Claim']}\n",
    "        for i, row in claim_df.iterrows() if int(row['Index']) in claim_indices]\n",
    "    \n",
    "    # Add original claim to front of shortlist\n",
    "    shortlist.insert(0, {\"rumourID\": int(claim_id), \"category\": claim_category, \"description\": claim_text})\n",
    "    \n",
    "    for claim in shortlist:\n",
    "        if claim[\"rumourID\"] not in known_claim_ids:\n",
    "            known_claim_ids.append(claim[\"rumourID\"])\n",
    "            known_claims[str(claim[\"rumourID\"])] = claim\n",
    "    \n",
    "    tweet_query[\"shortlist\"] = [claim[\"rumourID\"] for claim in shortlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(known_claims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Produce JSONs for the annotation form generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tweet_sample = [{\"tweetID\": str(tweet_query[\"id\"]), \"text\": tweet_query[\"text\"], \"rumourShortlist\": tweet_query[\"shortlist\"]} \n",
    "                for tweet_query in tweet_queries]\n",
    "print(json.dumps({\"tweetSample\": tweet_sample}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(known_claims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use the annotation form generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the JSONs from the above cell outputs into the form generator here: https://script.google.com/a/macros/sheffield.ac.uk/s/AKfycbyVFaLRlZrgQYsZTGPbBirRA6maY5WD1CGSZOHPS31l5ThHxQxoDtgIQssYXUSTVl3r/exec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
